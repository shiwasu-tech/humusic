{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初期設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "import icecream as ic\n",
    "import os\n",
    "from symusic import Score\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deviceの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device setting\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットとトークナイザーの設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizrの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keisu\\AppData\\Roaming\\Python\\Python39\\site-packages\\miditok\\tokenizations\\remi.py:77: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "# Creating a multitrack tokenizer, read the doc to explore all the parameters\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "tokenizer = REMI(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mac環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the tokenizer with Byte Pair Encoding (BPE)\n",
    "#files_paths = list(Path(\"../datasets\").glob(\"**/*.mid\"))\n",
    "dataset_dir = Path(\"/Users/hapticslab/Programming/humusic/datasets/orchestra\")\n",
    "#dataset_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/datasets/orchestra\")\n",
    "#dataset_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/datasets/piano1_only\")\n",
    "files_paths = list(dataset_dir.glob(\"**/*.mid\"))\n",
    "\n",
    "tokenizer.train(vocab_size=30000, files_paths=files_paths)\n",
    "\n",
    "#tokenizer.save(Path(\"path\", \"to\", \"save\", \"tokenizer.json\"))\n",
    "'''\n",
    "tokenizer.save(Path(\"/Users/hapticslab/Programming/humusic/midi_generator/tokens/orchestra_token/tokenizer.json\"))\n",
    "'''\n",
    "#tokenizer.save(Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/tokens/orchestra_token/tokenizer.json\"))\n",
    "#tokenizer.save(Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/tokens/piano1_token/tokenizer.json\"))\n",
    "# And pushing it to the Hugging Face hub (you can download it back with .from_pretrained)\n",
    "#tokenizer.push_to_hub(\"username/model-name\", private=True, token=\"your_hf_token\")\n",
    "\n",
    "# Split MIDIs into smaller chunks for training\n",
    "dataset_chunks_dir = Path(\"/Users/hapticslab/Programming/humusic/midi_generator/midi_chunk/orchestra\")\n",
    "#dataset_chunks_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/midi_chunk/orchestra\")\n",
    "#dataset_chunks_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/midi_chunk/piano1_only\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the tokenizer with Byte Pair Encoding (BPE)\n",
    "#files_paths = list(Path(\"../datasets\").glob(\"**/*.mid\"))\n",
    "#dataset_dir = Path(\"/Users/hapticslab/Programming/humusic/datasets/piano1_only\")\n",
    "#dataset_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/datasets/orchestra\")\n",
    "dataset_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/datasets/piano1_only\")\n",
    "files_paths = list(dataset_dir.glob(\"*.mid\"))\n",
    "\n",
    "tokenizer.train(vocab_size=30000, files_paths=files_paths)\n",
    "\n",
    "\n",
    "#tokenizer.save(Path(\"path\", \"to\", \"save\", \"tokenizer.json\"))\n",
    "\n",
    "#tokenizer.save(Path(\"/Users/hapticslab/Programming/humusic/midi_generator/tokens/orchestra_token/tokenizer.json\"))\n",
    "\n",
    "#tokenizer.save(Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/tokens/orchestra_token/tokenizer.json\"))\n",
    "tokenizer.save(Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/tokens/piano1_token/tokenizer.json\"))\n",
    "# And pushing it to the Hugging Face hub (you can download it back with .from_pretrained)\n",
    "#tokenizer.push_to_hub(\"username/model-name\", private=True, token=\"your_hf_token\")\n",
    "\n",
    "# Split MIDIs into smaller chunks for training\n",
    "#dataset_chunks_dir = Path(\"/Users/hapticslab/Programming/humusic/midi_generator/midi_chunk/piano1_only\")\n",
    "#dataset_chunks_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/midi_chunk/orchestra\")\n",
    "dataset_chunks_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/midi_chunk/piano1_only\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークンをDataloaderに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting music files (C:\\Users\\keisu\\Programming\\humor\\humusic\\midi_generator\\midi_chunk\\piano1_only): 100%|██████████| 283/283 [00:00<00:00, 718.22it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_files_for_training(\n",
    "    files_paths=files_paths,\n",
    "    tokenizer=tokenizer,\n",
    "    save_dir=dataset_chunks_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "\n",
    "# Create a Dataset, a DataLoader and a collator to train a model\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=list(dataset_chunks_dir.glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id, copy_inputs_as_labels=True)\n",
    "dataloader = DataLoader(dataset, batch_size=16, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention付きLSTMによる学習，生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# LSMT with Attention\n",
    "from LSTMwithAtt import LSTMwithAtt\n",
    "\n",
    "\n",
    "hidden_size = 512\n",
    "model = LSTMwithAtt(tokenizer, hidden_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "num_epoch = 30\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 Loss 0: 100%|██████████| 29/29 [00:50<00:00,  1.73s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/30 loss : 79.83838653564453 time : 50.16966104507446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 Loss 0: 100%|██████████| 29/29 [00:57<00:00,  1.97s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2/30 loss : 77.52143096923828 time : 57.09922742843628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 Loss 0: 100%|██████████| 29/29 [00:57<00:00,  1.98s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3/30 loss : 77.74165344238281 time : 57.33832001686096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 Loss 0: 100%|██████████| 29/29 [00:55<00:00,  1.92s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4/30 loss : 75.0734634399414 time : 55.56214666366577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 Loss 0: 100%|██████████| 29/29 [00:58<00:00,  2.00s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5/30 loss : 71.10094451904297 time : 58.01419472694397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 Loss 0: 100%|██████████| 29/29 [00:58<00:00,  2.02s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6/30 loss : 73.1193618774414 time : 58.56089997291565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 Loss 0: 100%|██████████| 29/29 [00:54<00:00,  1.87s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7/30 loss : 67.33294677734375 time : 54.131277084350586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 Loss 0: 100%|██████████| 29/29 [00:58<00:00,  2.02s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8/30 loss : 68.59640502929688 time : 58.54971623420715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 Loss 0: 100%|██████████| 29/29 [00:57<00:00,  1.98s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9/30 loss : 66.32911682128906 time : 57.41451835632324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 Loss 0: 100%|██████████| 29/29 [00:53<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10/30 loss : 64.76016235351562 time : 53.60330319404602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 Loss 0: 100%|██████████| 29/29 [00:56<00:00,  1.95s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11/30 loss : 61.701629638671875 time : 56.61405849456787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 Loss 0: 100%|██████████| 29/29 [00:57<00:00,  1.97s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12/30 loss : 62.938899993896484 time : 57.05817985534668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 Loss 0: 100%|██████████| 29/29 [00:53<00:00,  1.84s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13/30 loss : 57.908382415771484 time : 53.454461336135864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 Loss 0: 100%|██████████| 29/29 [00:56<00:00,  1.93s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14/30 loss : 60.50680160522461 time : 56.02928185462952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 Loss 0: 100%|██████████| 29/29 [00:57<00:00,  1.98s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15/30 loss : 54.385223388671875 time : 57.529913663864136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 Loss 0: 100%|██████████| 29/29 [00:53<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16/30 loss : 51.847808837890625 time : 53.90241813659668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 Loss 0: 100%|██████████| 29/29 [00:57<00:00,  1.97s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17/30 loss : 50.27029800415039 time : 57.01455783843994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 Loss 0: 100%|██████████| 29/29 [01:00<00:00,  2.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18/30 loss : 45.70622253417969 time : 60.936776638031006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 Loss 0: 100%|██████████| 29/29 [00:57<00:00,  1.97s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19/30 loss : 47.58204650878906 time : 57.07874250411987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 Loss 0: 100%|██████████| 29/29 [01:02<00:00,  2.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20/30 loss : 47.35494613647461 time : 62.40451622009277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 Loss 0: 100%|██████████| 29/29 [01:00<00:00,  2.09s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21/30 loss : 40.442787170410156 time : 60.654611110687256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 Loss 0: 100%|██████████| 29/29 [00:55<00:00,  1.91s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22/30 loss : 42.64940643310547 time : 55.46627068519592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 Loss 0: 100%|██████████| 29/29 [00:58<00:00,  2.01s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23/30 loss : 35.238155364990234 time : 58.42630958557129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 Loss 0: 100%|██████████| 29/29 [01:02<00:00,  2.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24/30 loss : 30.435945510864258 time : 62.46255397796631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 Loss 0: 100%|██████████| 29/29 [00:57<00:00,  1.97s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25/30 loss : 26.886943817138672 time : 57.120808839797974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 Loss 0: 100%|██████████| 29/29 [01:00<00:00,  2.08s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26/30 loss : 24.52595329284668 time : 60.180164098739624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 Loss 0: 100%|██████████| 29/29 [01:01<00:00,  2.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27/30 loss : 22.427440643310547 time : 61.89595365524292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 Loss 0: 100%|██████████| 29/29 [00:55<00:00,  1.92s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28/30 loss : 48.733760833740234 time : 55.732603788375854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 Loss 0: 100%|██████████| 29/29 [00:58<00:00,  2.00s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29/30 loss : 39.6092414855957 time : 58.097989082336426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 Loss 0: 100%|██████████| 29/29 [00:58<00:00,  2.02s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30/30 loss : 31.659929275512695 time : 58.57815861701965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    i = 0\n",
    "    loss  = 0\n",
    "    \n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epoch} |', unit='batch'):\n",
    "        encoder_input = batch[\"input_ids\"][:,1:].to(device)\n",
    "        decoder_input = batch[\"input_ids\"][:,:-1].to(device)\n",
    "        #labels = batch[\"labels\"][:,1:]\n",
    "        \n",
    "        e_input = pad_sequence(encoder_input, batch_first=True).to(device)\n",
    "        d_input = pad_sequence(decoder_input, batch_first=True).to(device)\n",
    "        ans = pad_sequence(labels, batch_first=True, padding_value=-100).to(device)\n",
    "        \n",
    "        #print(encoder_input.shape, decoder_input.shape, e_input.shape, d_input.shape)\n",
    "        \n",
    "        \n",
    "        labels = batch[\"input_ids\"][:,1:].to(device)    # 正解ラベル\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=tokenizer.vocab_size).float()\n",
    "        \n",
    "        #print(encoder_input.shape, decoder_input.shape)\n",
    "        \n",
    "        #out = model(e_input, d_input)\n",
    "        out = model(encoder_input, decoder_input)\n",
    "        \n",
    "        \n",
    "        #loss = criterion(out.view(-1, out.size(-1)), inputs.view(-1))\n",
    "        #print(out.shape,labels_one_hot.shape)\n",
    "        loss = criterion(out[0],labels_one_hot[0])\n",
    "        #loss = criterion(out[0],labels[0])\n",
    "        for h in range(1,len(out)):\n",
    "            loss += criterion(out[h],labels_one_hot[h])\n",
    "        #print(f'| epoch {epoch:3d} | {i:5d}/{len(dataloader)} batches | loss {loss.item():5.2f} | time {time.time()-epoch_start_time:5.2f}s')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "    \n",
    "    print(f'epoch {epoch+1}/{num_epoch} | loss {loss} | time {time.time()-epoch_start_time}')\n",
    "    outfile = \"models/\" + \"ltest\"+ str(epoch+1) + \".model\"\n",
    "    torch.save(model.state_dict(),outfile)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成-プロンプトのトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 17]),\n",
       " tensor([[  445,   767,   234, 10777,  2616,  2793,  2314,  2008,  1360,  1855,\n",
       "           1261,  2149,  1538,  4742,    58,  3483,  1005]], device='cuda:0'))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 入力トークン列の例\n",
    "prompt = (\"prompt.mid\")\n",
    "\n",
    "input_ids = torch.tensor([tokenizer(prompt)]).to(device)\n",
    "input_ids.shape, input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6877, 806, 20712, 8903, 356, 4538, 1735, 6327, 27082, 1727, 3876, 17888, 4489, 6327, 17098, 11337, 5718, 1735, 26813, 13134, 1727, 26823, 27082, 11514, 600, 806, 17098, 600, 8018, 279, 600, 600, 1788, 14379, 258, 11514, 600, 806, 17098, 600, 8018, 746, 506, 20871, 506, 20871, 600, 6398, 17888, 14854, 17098, 7027, 18233, 7027, 18219, 5741, 7027, 17888, 17888, 1238, 17888, 17888, 9896, 26813, 4695, 600, 1409, 17888, 9946, 5718, 11337, 600, 5718, 746, 20871, 27369, 600, 600, 600, 20596, 600, 600, 600, 7575, 2395, 258, 11514, 17098, 17039, 12279, 29709, 11337, 982, 691, 5741, 2555, 11514, 12799, 11514, 27369, 21632, 27688, 17973, 26619, 11514, 11514, 26050, 9273, 28241, 20871, 29709, 600, 600, 15096, 806, 806, 1029, 806, 23004, 27688, 12799, 11514, 11514, 17973, 511, 284, 20871, 18219, 6935, 5718, 17973, 9131, 1660, 746, 825, 11514, 600, 806, 600, 8018, 10102, 12799, 5322, 12799, 11514, 11514, 11514, 1788, 14828, 17973, 6398, 4989, 17888, 17888, 1760, 17888, 806, 600, 600, 600, 600, 600, 600, 600, 600, 600, 2617, 11337, 18233, 14854, 14379, 11514, 21632, 3876, 600, 26414, 279, 8018, 220, 6327, 22808, 806, 17888, 806, 12799, 11514, 12799, 11514, 12799, 11514, 11514, 17888, 11480, 27688, 1735, 2816, 5718, 29436, 21912, 1934, 3876, 17888, 18233, 25217, 600, 600, 2066, 9885, 10658, 1727, 29273, 14895, 907, 907, 746, 448, 1327, 600, 600, 600, 600, 17039, 907, 600, 19721, 25205, 2541, 20871, 4400, 10201, 2555, 17888, 17888, 17888, 9896, 26813, 29455, 11514, 12799, 5809, 17039, 14895, 4084, 17888, 5718, 11337, 14919, 22353, 4694, 12815, 600, 18233, 11337, 806, 746, 11480, 11514, 26050, 600, 8018, 907, 26813, 600, 600, 7575, 600, 600, 1735, 4188, 11337, 18233, 17974, 18233, 14854, 14379, 11514, 12799, 11514, 8718, 11514, 12857, 9930, 20871, 17973, 4989, 4989, 284, 258, 356, 29315, 17888, 1735, 10974, 23880, 27369, 8639, 258, 600, 600, 806, 279, 746, 9273, 11514, 600, 806, 600, 806, 600, 600, 806, 600, 8018, 11337, 600, 600, 7575, 806, 600, 1788, 15937, 7027, 29273, 513, 907, 1727, 26619, 3876, 17888, 16387, 17098, 600, 8018, 746, 7632, 600, 29709, 7575, 1727, 29709, 806, 9930, 1997, 29709, 14379, 17836, 16851, 691, 11514, 4884, 600, 27369, 17439, 15789, 2642, 513, 600, 806, 20874, 1368, 17098, 7027, 5607, 691, 27688, 3876, 5607, 8018, 17098, 600, 600, 27933, 1504, 18219, 18233, 17888, 17888, 1238, 17888, 17888, 9946, 6327, 600, 600, 2066, 1727, 27688, 806, 279, 691, 11514, 12799, 831, 11514, 22808, 12799, 11514, 11514, 17888, 11480, 27688, 1735, 4802, 17888, 29709, 600, 600, 600, 7575, 806, 600, 1788, 15937, 7027, 29273, 513, 907, 746, 600, 13514, 356, 600, 806, 279, 1727, 26619, 3876, 27688, 258, 2555, 11514, 600, 18146, 18146, 798, 20871, 17888, 4802, 279, 580, 21708, 17888, 17888, 14854, 2135, 27688, 26619, 27730, 825, 600, 600, 27933, 17098, 17098, 17098, 17098, 4697, 12514, 24860, 11480, 17888, 14128, 21708, 5607, 600, 5685, 806, 1727, 17098, 17098, 12799, 26619, 16344, 2555, 11514, 23880, 28303, 5718, 1416, 11514, 600, 806, 19721, 6327, 11480, 11514, 17836, 600, 24967, 1119, 600, 806, 17098, 600, 2066, 5640, 11514, 3080, 14718, 9273, 3876, 600, 600, 600, 806, 279, 279, 746, 9273, 26619, 11514, 12799, 5322, 5322, 8644, 19721, 2642, 17973, 600, 3876, 14895, 4084, 21246, 6398, 8018, 8718, 26619, 11514, 29709, 10270, 600, 14895, 17098, 17039, 27688, 4531, 18219, 7575, 7208, 14379, 600, 5718, 26813, 1055, 4584, 7027, 11526, 29455, 11514, 26050, 11502, 5741, 2541, 17888, 17888, 600, 600, 600, 1347, 600, 600, 1119, 22808, 600, 806, 600, 1788, 8055, 825, 8825, 600, 806, 600, 600, 5685, 17098, 12815, 825, 11514, 17836, 11480, 20871, 11439, 600, 806, 600, 1788, 6107, 29455, 11514, 10201, 600, 18233, 29315, 7027, 6398, 12815, 9273, 3876, 2555, 17888, 11514, 12799, 927, 12799, 11514, 11514, 17973, 29436, 18233, 3876, 6398, 21708, 806, 8185, 433, 600, 600, 806, 279, 279, 1788, 600, 806, 17098, 17888, 28593, 9896, 8018, 12759, 9739, 26619, 11514, 11514, 28241, 5951, 691, 20871, 600, 27369, 4902, 11514, 25205, 825, 258, 11514, 26405, 18219, 5718, 1047, 18233, 12514, 17098, 17234, 6398, 5752, 12799, 11514, 11514, 17888, 18219, 21708, 17888, 12815, 825, 806, 22808, 27933, 9273, 20871, 285, 11514, 12799, 11514, 27369, 27933, 1836, 17581, 5735, 29315, 11514, 12799, 11514, 12799, 11514, 11514, 11514, 26050, 600, 1238, 17888, 14854, 15937, 7027, 18219, 4989, 17888, 17888, 2559, 4697, 7632, 7632, 17888, 28593, 26619, 4884, 9739, 17888, 3888, 806, 1727, 14862, 3876, 600, 23004, 220, 600, 600, 7575, 806, 1727, 14862, 3876, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 9930, 600, 12657, 600, 27369, 408, 1727, 3876, 600, 806, 600, 10518, 20874, 982, 798, 12828, 600, 8018, 746, 26499, 3876, 600, 18233, 17888, 806, 279, 1788, 12799, 11514, 7261, 448, 3876, 17888, 29709, 26475, 4079, 17888, 9739, 6301, 26813, 11514, 17836, 600, 14128, 5735, 17888, 18233, 29315, 17888, 1735, 6877, 600, 600, 600, 7575, 806, 22808, 600, 600, 600, 12799, 825, 11514, 1727, 5322, 29709, 7575, 1727, 11514, 11514, 12799, 831, 1446, 27082, 4584, 220, 18942, 17888, 1788, 1027, 285, 11514, 12799, 26619, 23880, 11514, 11514, 1788, 4565, 10580, 18219, 4169, 14895, 600, 27369, 907, 691, 1027, 22808, 12799, 11514, 11514, 17888, 4232, 600, 3279, 11514, 26050, 15812, 5718, 279, 17888, 2816, 5718, 907, 1727, 3876, 29273, 11514, 17973, 23824, 15837, 22115, 21246, 29709, 600, 5685, 1727, 3876, 17039, 746, 806, 798, 3876, 600, 20596, 600, 8018, 17098, 600, 8018, 279, 600, 5685, 927, 1172, 11514, 600, 806, 17098, 600, 8018, 279, 600, 600, 5685, 17098, 29709, 18233, 19721, 17973, 12514, 19235, 1560, 26619, 9686, 17888, 580, 1409, 2555, 2555, 17888, 5743, 17888, 9896, 29436, 1047, 22808, 600, 600, 600, 2617, 12815, 17098, 12799, 26619, 23880, 7589, 11514, 11514, 600, 4086, 6925, 18146, 18146, 691, 10201, 20912, 6327, 11333, 4989, 600, 600, 14379, 11514, 600, 806, 17098, 600, 8018, 18219, 8018, 907, 600, 806, 17098, 12799, 11514, 12799, 909, 11514, 12799, 11514, 11514, 17973, 11337, 5718, 11337, 600, 27933, 1504, 27033, 26619, 4202, 806, 600, 600, 10518, 15630, 20871, 6327, 17888, 691, 4697, 14379, 927, 11514, 600, 806, 17098, 600, 8018, 7027, 8018, 17439, 17973, 17888, 14854, 29315, 6327, 27082, 1727, 26619, 4202, 14895, 17973, 17566, 279, 746, 17098, 7027, 8018, 1446, 9946, 14854, 1727, 26619, 3876, 17039, 17888, 17888, 806, 600, 12815, 17098, 600, 8018, 1327, 9131, 600, 12799, 1055, 11514, 17973, 8116, 26499, 9946, 9930, 27688, 18599, 746, 5322, 18599, 7249, 993, 11514, 18219, 29709, 8025, 20871, 6327, 927, 600, 806, 806, 806, 23126, 11526, 9498, 9739, 16783, 7027, 806, 600, 1788, 6877, 22808, 12799, 11514, 12799, 11514, 11514, 17973, 600, 14895, 3895, 17439, 20664, 25435, 20871, 6327, 1327, 22226, 11514, 600, 806, 17098, 600, 8018, 1416, 12799, 11514, 600, 806, 12799, 11514, 14997, 22808, 22808, 4538, 909, 224, 600, 600, 27933, 1504, 7027, 20596, 600, 5718, 17888, 17888, 17888, 14854, 29315, 9739, 11514, 29239, 6877, 600, 600, 600, 23004, 1735, 600, 27933, 9273, 6028, 27082, 1727, 11514, 17039, 26619, 19078, 27933, 10414, 29436, 25467, 1172, 11514, 17098, 17039, 17039, 27688, 600, 6877, 29455, 11514, 3876, 29436, 18233, 25217, 907, 9693, 7027, 8018, 825, 11514, 22808, 17098, 1735, 23014, 600, 3876, 17888, 600, 18233, 17098, 17098, 17098, 17098, 11480, 17888, 600, 21708, 3888, 691, 17888, 14128, 9273, 26619, 11514, 5640, 1727, 26619, 11514, 12799, 11514, 8718, 258, 11514, 11514, 600, 806, 600, 806, 17098, 7027, 18233, 7027, 806, 600, 600, 600, 600, 10534, 25295, 10201, 11514, 27369, 4538, 11337, 22808, 12799, 11514, 27369, 27933, 1836, 12799, 14895, 14379, 11514, 12799, 17039, 5809, 17039, 825, 20871, 17888, 17888, 11924, 691, 20696, 26619, 3274, 11514, 12799, 5322, 12799, 11514, 11514, 1788, 14828, 220, 18813, 18233, 17039, 11514, 17039, 285, 17888, 17888, 600, 600, 5374, 17888, 14973, 19721, 6327, 3876, 17888, 9896, 14128, 21708, 4802, 9896, 9896, 4012, 3876, 3876, 17888, 12815, 600, 8018, 13134, 27688, 1727, 29709, 18813, 1727, 26619, 4202, 806, 9930, 1172, 27688, 2555, 8185, 13134, 279, 17888, 17888, 1735, 806, 600, 806, 600, 806, 12799, 258, 356, 29315, 17888, 9739, 1735, 927, 600, 806, 17098, 1029, 806, 1760, 600, 1029, 3274, 7027, 600, 600, 1781, 29709, 22808, 29315, 17888, 806, 600, 1788, 14379, 1055, 20871, 17973, 12514, 4989, 17888, 17566, 15614, 1735, 5718, 1735, 28593, 17039, 600, 8143, 12799, 3876, 506, 2281, 2281, 2281, 600, 3876, 16395, 2617, 5718, 11337, 18233, 2555, 600, 600, 10518, 9273, 11514, 17039, 17039, 506, 831, 506, 3876, 433, 506, 17234, 29709, 5607, 691, 11514, 12799, 506, 11480, 17888, 11924, 691, 927, 600, 806, 17098, 17098, 12799, 909, 17888, 27369, 600, 600, 7575, 806, 600, 1788, 14379, 258, 11514, 17098, 600, 2066, 5640, 11514, 3080, 14718, 9273, 506, 17888, 17888, 17888, 600, 600, 2241, 18233, 8025, 1172, 11514, 17098, 17039, 17039, 17888, 17888, 11924, 15923, 907, 993, 11514, 600, 806, 600, 806, 600, 806, 600, 806, 600, 806, 600, 806, 12713, 21708, 18233, 27210, 600, 806, 279, 279, 746, 9273, 26619, 11514, 12799, 5322, 5322, 8644, 19721, 2642, 11514, 6767, 17973, 11514, 600, 806, 12799, 224, 4188, 14118, 25217, 825, 27688, 29709, 27933, 1047, 29937, 279, 1727, 3876, 29273, 17888, 17888, 9739, 11337, 1495, 17888, 600, 600, 600, 279, 1727, 8116, 7027, 7386, 14128, 26499, 3876, 600, 18233, 29315, 27688, 27688, 806, 1727, 11514, 17098, 11514, 26050, 13431, 18219, 8018, 600, 600, 1278, 18146, 20871, 22808, 4084, 220, 1037, 1055, 825, 4584, 220, 258, 21246, 1727, 5322, 29709, 7575, 1029, 806, 279, 746, 11480, 11514, 14973, 18146, 691, 29709, 4802, 9930, 600, 9930, 4584, 14997, 23880, 27369, 26414, 279, 600, 600, 2066, 18968, 1727, 3876, 17098, 17039, 258, 11514, 600, 806, 17098, 600, 8018, 11337, 806, 19721, 25570, 1356, 9896, 806, 12799, 11514, 17888, 506, 2281, 2281, 600, 29273, 9898, 11514, 26050, 600, 5607, 10514, 18060, 17039, 285, 11514, 600, 600, 26414, 746, 11480, 11514, 26050, 600, 8018, 20484, 806, 25451, 18146, 11439, 27369, 907, 17973, 11597, 20871, 12514, 19235, 10903, 1029, 806, 1727, 11514, 17098, 11514, 26050, 13431, 26405, 20894, 725, 17973, 17039, 27688, 600, 4084, 15438, 7208, 26619, 27933, 5322, 25208, 11936, 15210, 14895, 907, 746, 11514, 26050, 14973, 20871, 15893, 4989, 600, 18233, 25217, 806, 11924, 691, 17888, 14854, 29315, 806, 600, 600, 7575, 17439, 18233, 9221, 5971, 8183, 4538, 21767, 8183, 6327, 17888, 5718, 17888, 14854, 279, 600, 27933, 29315, 9885, 5201, 3876, 600, 600, 27933, 28199, 8639, 11514, 7163, 1727, 11514, 17039, 11514, 600, 806, 1727, 15203, 29709, 12773, 1727, 29709, 279, 279, 1727, 12828, 4985, 26050, 13102, 13063, 5607, 5607, 18233, 7027, 806, 600, 600, 600, 600, 600, 600, 600, 7208, 15199, 22835, 17888, 8018, 907, 4802, 17888, 4989, 17888, 4989, 17888, 2873, 26813, 600, 600, 23004, 600, 600, 600, 27933, 10731, 26813, 746, 4232, 7224, 20720, 3876, 29709, 600, 600, 600, 806, 17098, 11337, 18233, 17888, 5718, 11337, 22808, 12799, 746, 825, 11514, 17973, 26619, 3876, 3876, 220, 18233, 17098, 17098, 12799, 825, 11514, 17888, 17888, 600, 600, 17050, 12799, 20871, 12799, 11514, 11514, 11514, 1788, 14973, 393, 20871, 17973, 4802, 11514, 506, 600, 13514, 6747, 1327, 2281, 20871, 17888, 17888, 17888, 17888, 806, 600, 600, 600, 1347, 806, 600, 600, 600, 600, 600, 2617, 11337, 600, 7575, 806, 600, 5685, 982, 600, 806, 279, 20874, 2555, 258, 356, 29315, 17888, 1735, 806, 580, 11514, 600, 5935, 7027, 8018, 17973, 26813, 4400, 10201, 26619, 12699]\n"
     ]
    }
   ],
   "source": [
    "# Generate a MIDI file\n",
    "\n",
    "#model.load_state_dict(torch.load(\"models/lstmwithatttest30.model\"))\n",
    "model.load_state_dict(torch.load(\"models/ltest30.model\"))\n",
    "\n",
    "#torch.set_printoptions(threshold=torch.inf)\n",
    "model.eval()\n",
    "\n",
    "gen_token = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    #prompt_input = torch.LongTensor( input_ids ).to(device)\n",
    "    #print(input_ids.shape)\n",
    "    x = model.input_emb(input_ids)\n",
    "    #print(x.shape)\n",
    "    ox, (hnx, cnx) = model.lstm1(x)\n",
    "    #print(ox.shape, hnx.shape,cnx.shape)\n",
    "    hnx, cnx = hnx[:,0,:], cnx[:,0,:]\n",
    "    \n",
    "    wid = input_ids[0][0]\n",
    "    \n",
    "    sl = 0\n",
    "    while True:\n",
    "        wids = torch.LongTensor([wid]).to(device)\n",
    "        y = model.answer_emb(wids)\n",
    "        #print(y.shape)\n",
    "        \n",
    "        oy, (hnx, cnx) = model.lstm2(y, (hnx, cnx))\n",
    "        oy = oy.unsqueeze(1)\n",
    "        ox1 = ox.permute(0,2,1)\n",
    "        sim = torch.bmm(oy,ox1)\n",
    "        bs, yws, xws = sim.shape\n",
    "        sim2 = sim.reshape(bs*yws,xws)\n",
    "        alpha = F.softmax(sim2,dim=1).reshape(bs, yws, xws)\n",
    "        ct = torch.bmm(alpha,ox)\n",
    "        oy1 = torch.cat([ct,oy],dim=2)\n",
    "        oy2 = model.Wc(oy1)\n",
    "        oy3 = model.W(oy2)\n",
    "        wid = torch.argmax(oy3[0]).item()\n",
    "        gen_token.append(wid)\n",
    "        #if (wid == esid):\n",
    "            #break\n",
    "        if (sl == 2000):\n",
    "            break\n",
    "        sl += 1\n",
    "    print(gen_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated MIDI Tokens: Score(ttype=Tick, tpq=8, begin=0, end=1234, tracks=1, notes=1352, time_sig=1, key_sig=0, markers=0)\n"
     ]
    }
   ],
   "source": [
    "generated = tokenizer.decode(gen_token)\n",
    "print(\"Generated MIDI Tokens:\", generated)\n",
    "\n",
    "generated.dump_midi(\"generated_test10.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trasformerによる学習，生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=2048,\n",
    "    n_ctx=2048,\n",
    "    n_embd=256,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    \n",
    "    #bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    #eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    "    \n",
    "    )\n",
    "\n",
    "model = GPT2LMHeadModel(config).to(device)\n",
    "\n",
    "# 損失関数とオプティマイザ\n",
    "criterion = torch.nn.CrossEntropyLoss()  # トークンの予測タスクに使う\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)  # 適切な学習率を設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 29/29 [02:33<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Loss: 10.4803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 29/29 [02:26<00:00,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Loss: 9.8964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 学習ループ\n",
    "epochs = 2 # エポック数\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # モデルを学習モードに切り替え\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs = batch[\"input_ids\"].to(device)  # トークンID列\n",
    "        labels = batch[\"labels\"].to(device)    # 正解ラベル\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)  # マスク（任意）\n",
    "\n",
    "        # 勾配を初期化\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # モデルの前方計算\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        loss = outputs.loss  # GPT2LMHeadModelは自動で損失を計算する\n",
    "\n",
    "        # 勾配の計算とパラメータの更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 損失を記録\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # エポックごとの平均損失を出力\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    model.save_pretrained(\"models/piano1_only/test\" + str(epoch+1) + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成-プロンプトのトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  445,   767,   234, 10777,  2616,  2793,  2314,  2008,  1360,  1855,\n",
       "          1261,  2149,  1538,  4742,    58,  3483,  1005]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 入力トークン列の例\n",
    "prompt = (\"prompt.mid\")\n",
    "\n",
    "input_ids = torch.tensor([tokenizer(prompt)]).to(device)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成-メインプロセス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keisu\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `2.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\keisu\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\keisu\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\configuration_utils.py:537: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `200` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#regenerate\n",
    "\n",
    "# モデルの読み込み\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"models/piano1_only/test2.pt\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# モデルによる生成\n",
    "# output = model.generate(\n",
    "#     input_ids=input_ids,\n",
    "#     max_length=1024,\n",
    "#     num_beams=5,\n",
    "#     no_repeat_ngram_size=2,\n",
    "# )\n",
    "\n",
    "# モデルによる生成\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=1024,\n",
    "    num_beams=5,  # ビームサーチ\n",
    "    no_repeat_ngram_size=2,  # 同じn-gramを繰り返さない\n",
    "    temperature=2.0,  # サンプリングの多様性を調整\n",
    "    top_k=200,  # 上位k個のトークンをサンプリング\n",
    "    top_p=0.90,  # 累積確率cutoff\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成-結果をdetokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated MIDI Tokens: Score(ttype=Tick, tpq=8, begin=0, end=5480, tracks=1, notes=47, time_sig=1, key_sig=0, markers=0)\n"
     ]
    }
   ],
   "source": [
    "# トークン列をデコードして結果を表示\n",
    "generated = tokenizer.decode(output[0].tolist())\n",
    "print(\"Generated MIDI Tokens:\", generated)\n",
    "\n",
    "generated.dump_midi(\"generated_test5.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python(GPU)のキャッシュクリア"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model, dataset, dataloader, collator\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成したmidiの再生\n",
    "強制終了するとipykernelがクラッシュするので注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pygameによる再生\n",
    "\n",
    "file_name = Path(\"generated_test10.mid\")\n",
    "\n",
    "import time\n",
    "import pygame\n",
    "\n",
    "\n",
    "'''\n",
    "# Play the MIDI file in the current directory\n",
    "for msg in mid.play():\n",
    "    time.sleep(msg.time)\n",
    "    if not msg.is_meta:\n",
    "        print(msg)\n",
    "'''\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Set up the mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load the MIDI file\n",
    "#pygame.mixer.music.load('bach_850.mid')\n",
    "pygame.mixer.music.load(file_name)\n",
    "#pygame.mixer.music.load('test.mid')\n",
    "\n",
    "# Play the MIDI file\n",
    "pygame.mixer.music.set_volume(0.25)\n",
    "pygame.mixer.music.play()\n",
    "\n",
    "\n",
    "# Keep the program running until the music stops\n",
    "while pygame.mixer.music.get_busy():\n",
    "    if input(\"Press 'q' to quit: \") == 'q':\n",
    "        break\n",
    "    pygame.time.Clock().tick(10)\n",
    "\n",
    "# Quit pygame\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt.midの作成セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まずは1小節のnote（ド、C4）を1つ生成してみる\n",
    "import mido\n",
    "from mido import Message, MidiFile, MidiTrack, MetaMessage\n",
    "\n",
    "mid = MidiFile()\n",
    "track = MidiTrack()\n",
    "mid.tracks.append(track)\n",
    "track.append(MetaMessage('set_tempo', tempo=mido.bpm2tempo(120))) # bpm120\n",
    "\n",
    "\n",
    "track.append(Message('note_on', note=60, velocity=100, time=0))\n",
    "track.append(Message('note_off', note=60, velocity=100, time=480))\n",
    "track.append(Message('note_on', note=74, velocity=57, time=120))\n",
    "track.append(Message('note_on', note=76, velocity=57, time=120))\n",
    "track.append(Message('note_off', note=74, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=78, velocity=57, time=100))\n",
    "track.append(Message('note_off', note=76, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=69, velocity=60, time=100))\n",
    "track.append(Message('note_off', note=78, velocity=0, time=20))\n",
    "track.append(Message('note_off', note=69, velocity=0, time=100))\n",
    "track.append(Message('note_on', note=78, velocity=57, time=0))\n",
    "track.append(Message('note_on', note=76, velocity=57, time=120))\n",
    "track.append(Message('note_off', note=78, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=74, velocity=57, time=100))\n",
    "track.append(Message('note_off', note=76, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=81, velocity=62, time=100))\n",
    "track.append(Message('note_off', note=74, velocity=0, time=20))\n",
    "track.append(Message('note_off', note=81, velocity=0, time=100))\n",
    "track.append(Message('note_on', note=74, velocity=58, time=0))\n",
    "track.append(Message('note_on', note=76, velocity=58, time=120))\n",
    "track.append(Message('note_off', note=74, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=78, velocity=58, time=100))\n",
    "track.append(Message('note_off', note=76, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=69, velocity=61, time=100))\n",
    "track.append(Message('note_off', note=78, velocity=0, time=20))\n",
    "track.append(Message('note_off', note=69, velocity=0, time=100))\n",
    "track.append(Message('note_on', note=78, velocity=57, time=0))\n",
    "track.append(Message('note_on', note=76, velocity=57, time=120))\n",
    "track.append(Message('note_off', note=78, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=74, velocity=57, time=100))\n",
    "track.append(Message('note_off', note=76, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=79, velocity=60, time=100))\n",
    "track.append(Message('note_off', note=74, velocity=0, time=20))\n",
    "\n",
    "\n",
    "mid.save('prompt.mid') # MidiFileを保存\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
