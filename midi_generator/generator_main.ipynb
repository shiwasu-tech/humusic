{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keisu\\AppData\\Roaming\\Python\\Python39\\site-packages\\miditok\\tokenizations\\remi.py:77: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Creating a multitrack tokenizer, read the doc to explore all the parameters\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "tokenizer = REMI(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTMwithAtt import LSTMwithAtt\n",
    "import torch\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "\n",
    "import icecream as ic\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from symusic import Score\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm  # 進行状況を可視化するライブラリ\n",
    "\n",
    "from transformers import GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device setting\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer with Byte Pair Encoding (BPE)\n",
    "#files_paths = list(Path(\"../datasets\").glob(\"**/*.mid\"))\n",
    "# dataset_dir = Path(\"/Users/hapticslab/Programming/humusic/datasets/orchestra\")\n",
    "dataset_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/datasets/orchestra\")\n",
    "dataset_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/datasets/piano1_only\")\n",
    "files_paths = list(dataset_dir.glob(\"**/*.mid\"))\n",
    "\n",
    "tokenizer.train(vocab_size=30000, files_paths=files_paths)\n",
    "\n",
    "#tokenizer.save(Path(\"path\", \"to\", \"save\", \"tokenizer.json\"))\n",
    "# tokenizer.save(Path(\"/Users/hapticslab/Programming/humusic/midi_generator/tokens/token_folder_name/tokenizer.json\"))\n",
    "#tokenizer.save(Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/tokens/orchestra_token/tokenizer.json\"))\n",
    "tokenizer.save(Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/tokens/piano1_token/tokenizer.json\"))\n",
    "# And pushing it to the Hugging Face hub (you can download it back with .from_pretrained)\n",
    "#tokenizer.push_to_hub(\"username/model-name\", private=True, token=\"your_hf_token\")\n",
    "\n",
    "# Split MIDIs into smaller chunks for training\n",
    "# dataset_chunks_dir = Path(\"/Users/hapticslab/Programming/humusic/midi_generator/midi_chunk\")\n",
    "#dataset_chunks_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/midi_chunk/orchestra\")\n",
    "dataset_chunks_dir = Path(\"C:/Users/keisu/Programming/humor/humusic/midi_generator/midi_chunk/piano1_only\")\n",
    "\n",
    "'''\n",
    "split_files_for_training(\n",
    "    files_paths=files_paths,\n",
    "    tokenizer=tokenizer,\n",
    "    save_dir=dataset_chunks_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "'''\n",
    "# Create a Dataset, a DataLoader and a collator to train a model\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=list(dataset_chunks_dir.glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id, copy_inputs_as_labels=True)\n",
    "dataloader = DataLoader(dataset, batch_size=16, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSMT with Attention\n",
    "'''\n",
    "hidden_size = 200\n",
    "model = LSTMwithAtt(4, hidden_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "num_epoch = 10\n",
    "\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    i = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        xs, ys ,ans = [],[],[]\n",
    "\n",
    "        for k in range(len(batch)):\n",
    "            \n",
    "            input = batch[\"input_ids\"].to(device)\n",
    "            \n",
    "            tid = batch[k]\n",
    "            xs.append(torch.LongTensor(tid[1:]))\n",
    "            ys.append(torch.LongTensor(tid[:-1]))\n",
    "            ans.append(torch.LongTensor(tid[1:]))\n",
    "        \n",
    "        encoder_input = pad_sequence(xs, batch_first=True).to(device)\n",
    "        decoder_input = pad_sequence(ys, batch_first=True).to(device)\n",
    "        answer = pad_sequence(ans, batch_first=True, padding_value=-1.0).to(device)\n",
    "\n",
    "        out = model(encoder_input, decoder_input)\n",
    "        loss = criterion(out[0],answer[0])\n",
    "        for h in range(1,len(out)):\n",
    "            loss += criterion(out[h],answer[h])\n",
    "        print(f'| epoch {epoch:3d} | {i:5d}/{len(dataloader)} batches | loss {loss.item():5.2f} | time {time.time()-epoch_start_time:5.2f}s')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "    \n",
    "    print(f'epoch {epoch+1}/{num_epoch} time : {time.time()-epoch_start_time}')\n",
    "    outfile = \"models/\" + \"test\"+ str(epoch+1) + \".model\"\n",
    "    torch.save(model.state_dict(),outfile)'''\n",
    "\n",
    "None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=2048,\n",
    "    n_ctx=2048,\n",
    "    n_embd=256,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    \n",
    "    #bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    #eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    "    \n",
    "    )\n",
    "\n",
    "model = GPT2LMHeadModel(config).to(device)\n",
    "\n",
    "# 損失関数とオプティマイザ\n",
    "criterion = torch.nn.CrossEntropyLoss()  # トークンの予測タスクに使う\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)  # 適切な学習率を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 29/29 [02:33<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Loss: 10.4803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 29/29 [02:26<00:00,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Loss: 9.8964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 学習ループ\n",
    "epochs = 2 # エポック数\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # モデルを学習モードに切り替え\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs = batch[\"input_ids\"].to(device)  # トークンID列\n",
    "        labels = batch[\"labels\"].to(device)    # 正解ラベル\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)  # マスク（任意）\n",
    "\n",
    "        # 勾配を初期化\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # モデルの前方計算\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        loss = outputs.loss  # GPT2LMHeadModelは自動で損失を計算する\n",
    "\n",
    "        # 勾配の計算とパラメータの更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 損失を記録\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # エポックごとの平均損失を出力\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    model.save_pretrained(\"models/piano1_only/test\" + str(epoch+1) + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### promptの選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  445,   767,   234, 10777,  2616,  2793,  2314,  2008,  1360,  1855,\n",
       "          1261,  2149,  1538,  4742,    58,  3483,  1005]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 入力トークン列の例\n",
    "prompt = (\"prompt.mid\")\n",
    "\n",
    "input_ids = torch.tensor([tokenizer(prompt)]).to(device)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keisu\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `2.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\keisu\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\keisu\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\configuration_utils.py:537: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `200` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#regenerate\n",
    "\n",
    "# モデルの読み込み\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"models/piano1_only/test2.pt\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# モデルによる生成\n",
    "# output = model.generate(\n",
    "#     input_ids=input_ids,\n",
    "#     max_length=1024,\n",
    "#     num_beams=5,\n",
    "#     no_repeat_ngram_size=2,\n",
    "# )\n",
    "\n",
    "# モデルによる生成\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=1024,\n",
    "    num_beams=5,  # ビームサーチ\n",
    "    no_repeat_ngram_size=2,  # 同じn-gramを繰り返さない\n",
    "    temperature=2.0,  # サンプリングの多様性を調整\n",
    "    top_k=200,  # 上位k個のトークンをサンプリング\n",
    "    top_p=0.90,  # 累積確率cutoff\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated MIDI Tokens: Score(ttype=Tick, tpq=8, begin=0, end=5480, tracks=1, notes=47, time_sig=1, key_sig=0, markers=0)\n"
     ]
    }
   ],
   "source": [
    "# トークン列をデコードして結果を表示\n",
    "generated = tokenizer.decode(output[0].tolist())\n",
    "print(\"Generated MIDI Tokens:\", generated)\n",
    "\n",
    "generated.dump_midi(\"generated_test5.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, dataset, dataloader, collator\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pygameによる再生\n",
    "\n",
    "import time\n",
    "import pygame\n",
    "\n",
    "\n",
    "'''\n",
    "# Play the MIDI file in the current directory\n",
    "for msg in mid.play():\n",
    "    time.sleep(msg.time)\n",
    "    if not msg.is_meta:\n",
    "        print(msg)\n",
    "'''\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Set up the mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load the MIDI file\n",
    "#pygame.mixer.music.load('bach_850.mid')\n",
    "pygame.mixer.music.load(\"generated_test5.mid\")\n",
    "#pygame.mixer.music.load('test.mid')\n",
    "\n",
    "# Play the MIDI file\n",
    "pygame.mixer.music.set_volume(0.25)\n",
    "pygame.mixer.music.play()\n",
    "\n",
    "\n",
    "# Keep the program running until the music stops\n",
    "while pygame.mixer.music.get_busy():\n",
    "    \n",
    "    pygame.time.Clock().tick(10)\n",
    "\n",
    "# Quit pygame\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test.midの作成セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まずは1小節のnote（ド、C4）を1つ生成してみる\n",
    "import mido\n",
    "from mido import Message, MidiFile, MidiTrack, MetaMessage\n",
    "\n",
    "mid = MidiFile()\n",
    "track = MidiTrack()\n",
    "mid.tracks.append(track)\n",
    "track.append(MetaMessage('set_tempo', tempo=mido.bpm2tempo(120))) # bpm120\n",
    "\n",
    "\n",
    "track.append(Message('note_on', note=60, velocity=100, time=0))\n",
    "track.append(Message('note_off', note=60, velocity=100, time=480))\n",
    "track.append(Message('note_on', note=74, velocity=57, time=120))\n",
    "track.append(Message('note_on', note=76, velocity=57, time=120))\n",
    "track.append(Message('note_off', note=74, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=78, velocity=57, time=100))\n",
    "track.append(Message('note_off', note=76, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=69, velocity=60, time=100))\n",
    "track.append(Message('note_off', note=78, velocity=0, time=20))\n",
    "track.append(Message('note_off', note=69, velocity=0, time=100))\n",
    "track.append(Message('note_on', note=78, velocity=57, time=0))\n",
    "track.append(Message('note_on', note=76, velocity=57, time=120))\n",
    "track.append(Message('note_off', note=78, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=74, velocity=57, time=100))\n",
    "track.append(Message('note_off', note=76, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=81, velocity=62, time=100))\n",
    "track.append(Message('note_off', note=74, velocity=0, time=20))\n",
    "track.append(Message('note_off', note=81, velocity=0, time=100))\n",
    "track.append(Message('note_on', note=74, velocity=58, time=0))\n",
    "track.append(Message('note_on', note=76, velocity=58, time=120))\n",
    "track.append(Message('note_off', note=74, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=78, velocity=58, time=100))\n",
    "track.append(Message('note_off', note=76, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=69, velocity=61, time=100))\n",
    "track.append(Message('note_off', note=78, velocity=0, time=20))\n",
    "track.append(Message('note_off', note=69, velocity=0, time=100))\n",
    "track.append(Message('note_on', note=78, velocity=57, time=0))\n",
    "track.append(Message('note_on', note=76, velocity=57, time=120))\n",
    "track.append(Message('note_off', note=78, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=74, velocity=57, time=100))\n",
    "track.append(Message('note_off', note=76, velocity=0, time=20))\n",
    "track.append(Message('note_on', note=79, velocity=60, time=100))\n",
    "track.append(Message('note_off', note=74, velocity=0, time=20))\n",
    "\n",
    "\n",
    "mid.save('prompt.mid') # MidiFileを保存\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
