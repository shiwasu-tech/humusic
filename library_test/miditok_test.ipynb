{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  miditok import REMI, TokenizerConfig\n",
    "from symusic import Score\n",
    "import icecream as ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keisu\\AppData\\Roaming\\Python\\Python39\\site-packages\\miditok\\tokenizations\\remi.py:77: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "# Creating a multitrack tokenizer, read the doc to explore all the parameters\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# Loads a midi, converts to tokens, and back to a MIDI\n",
    "midi = Score(\"bach_850.mid\")\n",
    "tokens = tokenizer(midi)  # calling the tokenizer will automatically detect MIDIs, paths and tokens\n",
    "converted_back_midi = tokenizer(tokens)  # PyTorch, Tensorflow and Numpy tensors are supported\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting music files (C:\\Users\\keisu\\Programming\\humusic\\path\\to\\midi_chunks): 100%|██████████| 295/295 [00:01<00:00, 291.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n"
     ]
    }
   ],
   "source": [
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Creating a multitrack tokenizer, read the doc to explore all the parameters\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# Train the tokenizer with Byte Pair Encoding (BPE)\n",
    "#files_paths = list(Path(\"../datasets\").glob(\"**/*.mid\"))\n",
    "dataset_dir = Path(\"C:/Users/keisu/Programming/humusic/datasets/orchestra\")\n",
    "files_paths = list(dataset_dir.glob(\"**/*.mid\"))\n",
    "\n",
    "tokenizer.train(vocab_size=30000, files_paths=files_paths)\n",
    "#tokenizer.save(Path(\"path\", \"to\", \"save\", \"tokenizer.json\"))\n",
    "tokenizer.save(Path(\"C:/Users/keisu/Programming/humusic/path/to/save/tokenizer.json\"))\n",
    "# And pushing it to the Hugging Face hub (you can download it back with .from_pretrained)\n",
    "#tokenizer.push_to_hub(\"username/model-name\", private=True, token=\"your_hf_token\")\n",
    "\n",
    "# Split MIDIs into smaller chunks for training\n",
    "dataset_chunks_dir = Path(\"C:/Users/keisu/Programming/humusic/path/to/midi_chunks\")\n",
    "\n",
    "split_files_for_training(\n",
    "    files_paths=files_paths,\n",
    "    tokenizer=tokenizer,\n",
    "    save_dir=dataset_chunks_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "\n",
    "# Create a Dataset, a DataLoader and a collator to train a model\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=list(dataset_chunks_dir.glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id, copy_inputs_as_labels=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64, collate_fn=collator)\n",
    "\n",
    "print(\"tokenize complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
